---
title: "Practical_Machine_Learning Project"
output: html_document
---


## 1. Executive Summary

This document presents the results of the Practical Machine Learning Peer Assessments in a report using a single R markdown document.

Since I have a data set with to many columns(160) and it needs to make a class prediction, I select a random forests model as a learning method, that’s no need cross-validation or a separate test set to get an unbiased estimate of the test set error. 
Before apply the dataset to this prediction model, I decide remove all the columns that having less than 80% of data filled, instead try to filled it with some center measure. 
This model accuracy over validation dataset is equal to 99.9235%. This model promoted a excelente prediction results with testing dataset and generated the 20th files answers to submit for the Assignments. 


## 2. Problem explanation 

The goal of this project is to :

1.  predict the manner in which they did the exercise. 
2. This is the "classe" variable in the training set. All other variables can be use as predictor.
3. It needs to be described how to built prediction model, how to use cross validation, what sample error is expected out, and why the choices was maded. 
4. It will be used also use prediction model to predict 20 different test cases. 

#### Dataset - HAR

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. 

## 2. Data Analysis 

#### 2.1 Exploratoty Data Analysis 

##### Data Loading and Cleaning

```{r cache=TRUE,warning=FALSE, message=FALSE} 
library(caret)
library(rpart)
library(randomForest)
library(Formula)
# data loading
training<-read.csv("pml-training.csv",na.strings=c("NA",""))
training<-training[,-1] # Remove the first column that represents a ID Row 
``` 

- This data have total `r ncol(training)` variables.
- But several vairabls do not have values(missing values).
  First we check if we have many problems with columns without data. 

##### Number of columns with less than 90% of data

```{r,cache=TRUE,warning=FALSE}
sum((colSums(!is.na(training[,-ncol(training)])) < 0.9*nrow(training)))
```

- So, we apply our definition of remove columns that most doesn’t have data, before its apply to the model.

```{r,cache=TRUE,warning=FALSE}
Keep <- c((colSums(!is.na(training[,-ncol(training)])) >=0.9*nrow(training)))
training   <-  training[,Keep]
```

- New data have `r ncol(training)` variables.

##### Data Sets Partitions Definitions

- Create the data partitions of training and validating data sets from original training data sets.

```{r,cache=TRUE,warning=FALSE}
set.seed(357)
inTrain = createDataPartition(training$classe, p=0.60, list=FALSE)
validating = training[-inTrain,]
training = training[inTrain,]
```

#### 2.2 Data Modeling

I select random forests as a modeling of this project.
In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the execution. So, I proced with the training the model (Random Forest) with the training data set.

Due to extremely long running time with caret’s train function(1 hour>~), randomForest of ` randomForest` package are chosen(~1 min) for this project.


```{r,cache=TRUE,warning=FALSE}
modelfit <- randomForest(classe~.,data=training)
##modelfit <- train(classe~.,data=training,method="rf",prox=TRUE )
print(modelfit)
plot(modelfit)
#importance(modelfit)
varImpPlot(modelfit)
```

You can see figure with the verification of variable importance measures as produced by random Forest.

```{r,cache=TRUE,warning=FALSE}
confusionMatrix(predict(modelfit,validating[,-ncol(validating)]),validating$classe)
```

I confirmed the accurancy at validating data set by calculate it with the following formula:

```{r,cache=TRUE,warning=FALSE}
accuracy<-c(as.numeric((predict(modelfit,validating[,-ncol(validating)])==validating$classe)))
accuracy<-sum(accuracy)*100/nrow(validating)
```
Model Accuracy as tested over Validation set = `r accuracy`%.


## 3. Model Test

Finaly, I prossed with predicting the new values in the testing csv provided, first I apply the same data cleannig operations on it and coerce all columns of Testing data set for the same class of previuos data set.


#### 3.1 Apply the Same Transformations between testing and training data

```{r,cache=TRUE,warning=FALSE}

testing<-read.csv("pml-testing.csv",na.strings=c("NA",""))
testing<-testing[,-1]
testing <- testing[,Keep]
testing<- testing[,-ncol(testing)] #remove problem ID

# Coerce testing dataset to same class and strucuture of training dataset 
class_check <- (sapply(testing, class) == sapply(training, class))
testing[, !class_check] <- sapply(testing[, !class_check], as.numeric)
testing <- rbind(training[100, -59] ,testing) 
testing <-testing[-1,]
```

#### 3.2 Prediciting with Testing Dataset

```{r,cache=TRUE,warning=FALSE}
predictions<-predict(modelfit,testing)
print(predictions)
```

#### 3.3 Generating Answers Files to Submit for Assignment

```{r,cache=TRUE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(predictions)
```

Since the greatest accuracy level of my model, as expect, all 20th files answers submitted were correct!